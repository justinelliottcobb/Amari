//! Optimization Algorithms with Verified Mathematics
//!
//! This example demonstrates various optimization algorithms using
//! dual numbers for exact gradient computation and geometric algebra
//! for robust mathematical operations.

use amari_dual::{Dual, DualNumber};
use amari_core::{Vector, Multivector};
use rand::Rng;
use std::f64::consts::PI;

/// A general optimization problem using dual numbers
#[derive(Debug, Clone)]
pub struct OptimizationProblem {
    name: String,
    dimension: usize,
}

impl OptimizationProblem {
    /// Create a new optimization problem
    pub fn new(name: &str, dimension: usize) -> Self {
        Self {
            name: name.to_string(),
            dimension,
        }
    }

    /// Rosenbrock function: f(x,y) = (a-x)¬≤ + b(y-x¬≤)¬≤
    /// Global minimum at (a,a¬≤) with f(a,a¬≤) = 0
    pub fn rosenbrock(&self, variables: &[Dual<f64>]) -> Dual<f64> {
        assert_eq!(variables.len(), 2);
        let x = variables[0];
        let y = variables[1];
        let a = Dual::constant(1.0);
        let b = Dual::constant(100.0);

        let term1 = a.subtract(&x).square();
        let term2 = y.subtract(&x.square()).square().multiply(&b);
        term1.add(&term2)
    }

    /// Himmelblau's function: f(x,y) = (x¬≤+y-11)¬≤ + (x+y¬≤-7)¬≤
    /// Has four global minima
    pub fn himmelblau(&self, variables: &[Dual<f64>]) -> Dual<f64> {
        assert_eq!(variables.len(), 2);
        let x = variables[0];
        let y = variables[1];

        let term1 = x.square().add(&y).subtract(&Dual::constant(11.0)).square();
        let term2 = x.add(&y.square()).subtract(&Dual::constant(7.0)).square();
        term1.add(&term2)
    }

    /// Rastrigin function: f(x) = A*n + Œ£[x·µ¢¬≤ - A*cos(2œÄ*x·µ¢)]
    /// Highly multimodal with many local minima
    pub fn rastrigin(&self, variables: &[Dual<f64>]) -> Dual<f64> {
        let a = 10.0;
        let n = variables.len() as f64;
        let mut sum = Dual::constant(a * n);

        for &xi in variables {
            let cos_term = Dual::constant(2.0 * PI).multiply(&xi).cos();
            let term = xi.square().subtract(&Dual::constant(a).multiply(&cos_term));
            sum = sum.add(&term);
        }

        sum
    }

    /// Sphere function: f(x) = Œ£ x·µ¢¬≤
    /// Simple convex function with global minimum at origin
    pub fn sphere(&self, variables: &[Dual<f64>]) -> Dual<f64> {
        let mut sum = Dual::constant(0.0);
        for &xi in variables {
            sum = sum.add(&xi.square());
        }
        sum
    }

    /// Ackley function: f(x) = -a*exp(-b*‚àö(Œ£x·µ¢¬≤/n)) - exp(Œ£cos(c*x·µ¢)/n) + a + e
    pub fn ackley(&self, variables: &[Dual<f64>]) -> Dual<f64> {
        let a = 20.0;
        let b = 0.2;
        let c = 2.0 * PI;
        let n = variables.len() as f64;

        let sum_squares = variables.iter().fold(Dual::constant(0.0), |acc, &xi| acc.add(&xi.square()));
        let sum_cos = variables.iter().fold(Dual::constant(0.0), |acc, &xi| acc.add(&Dual::constant(c).multiply(&xi).cos()));

        let term1 = Dual::constant(-a).multiply(&Dual::constant(-b).multiply(&sum_squares.scale(1.0/n).sqrt()).exp());
        let term2 = sum_cos.scale(1.0/n).exp().scale(-1.0);
        let constant_term = Dual::constant(a + std::f64::consts::E);

        term1.add(&term2).add(&constant_term)
    }

    /// Compute gradient at a point using dual numbers
    pub fn compute_gradient(&self, point: &[f64]) -> Vec<f64> {
        let mut gradient = Vec::with_capacity(point.len());

        for i in 0..point.len() {
            let mut dual_vars: Vec<Dual<f64>> = point.iter()
                .enumerate()
                .map(|(j, &x)| if i == j { Dual::variable(x) } else { Dual::constant(x) })
                .collect();

            let result = match self.name.as_str() {
                "rosenbrock" => self.rosenbrock(&dual_vars),
                "himmelblau" => self.himmelblau(&dual_vars),
                "rastrigin" => self.rastrigin(&dual_vars),
                "sphere" => self.sphere(&dual_vars),
                "ackley" => self.ackley(&dual_vars),
                _ => self.sphere(&dual_vars),
            };

            gradient.push(result.dual());
        }

        gradient
    }

    /// Evaluate function value at a point
    pub fn evaluate(&self, point: &[f64]) -> f64 {
        let dual_vars: Vec<Dual<f64>> = point.iter().map(|&x| Dual::constant(x)).collect();

        let result = match self.name.as_str() {
            "rosenbrock" => self.rosenbrock(&dual_vars),
            "himmelblau" => self.himmelblau(&dual_vars),
            "rastrigin" => self.rastrigin(&dual_vars),
            "sphere" => self.sphere(&dual_vars),
            "ackley" => self.ackley(&dual_vars),
            _ => self.sphere(&dual_vars),
        };

        result.real()
    }
}

/// Gradient descent optimizer with exact gradients
#[derive(Debug, Clone)]
pub struct GradientDescent {
    learning_rate: f64,
    max_iterations: usize,
    tolerance: f64,
}

impl GradientDescent {
    /// Create a new gradient descent optimizer
    pub fn new(learning_rate: f64, max_iterations: usize, tolerance: f64) -> Self {
        Self {
            learning_rate,
            max_iterations,
            tolerance,
        }
    }

    /// Optimize a function starting from an initial point
    pub fn optimize(&self, problem: &OptimizationProblem, initial_point: Vec<f64>) -> OptimizationResult {
        let mut current_point = initial_point;
        let mut history = Vec::new();
        let mut iteration = 0;

        for iter in 0..self.max_iterations {
            iteration = iter;
            let function_value = problem.evaluate(&current_point);
            let gradient = problem.compute_gradient(&current_point);

            history.push(IterationData {
                iteration: iter,
                point: current_point.clone(),
                function_value,
                gradient_norm: gradient.iter().map(|&g| g * g).sum::<f64>().sqrt(),
            });

            // Check convergence
            let gradient_norm = gradient.iter().map(|&g| g * g).sum::<f64>().sqrt();
            if gradient_norm < self.tolerance {
                break;
            }

            // Update point: x = x - Œ±‚àáf
            for (xi, gi) in current_point.iter_mut().zip(gradient.iter()) {
                *xi -= self.learning_rate * gi;
            }
        }

        OptimizationResult {
            final_point: current_point,
            final_value: problem.evaluate(&history.last().unwrap().point),
            iterations: iteration + 1,
            converged: history.last().unwrap().gradient_norm < self.tolerance,
            history,
        }
    }
}

/// Adam optimizer with verified gradients
#[derive(Debug, Clone)]
pub struct AdamOptimizer {
    learning_rate: f64,
    beta1: f64,
    beta2: f64,
    epsilon: f64,
    max_iterations: usize,
    tolerance: f64,
}

impl AdamOptimizer {
    /// Create a new Adam optimizer
    pub fn new(learning_rate: f64, max_iterations: usize, tolerance: f64) -> Self {
        Self {
            learning_rate,
            beta1: 0.9,
            beta2: 0.999,
            epsilon: 1e-8,
            max_iterations,
            tolerance,
        }
    }

    /// Optimize using Adam algorithm
    pub fn optimize(&self, problem: &OptimizationProblem, initial_point: Vec<f64>) -> OptimizationResult {
        let mut current_point = initial_point;
        let mut m = vec![0.0; current_point.len()]; // First moment
        let mut v = vec![0.0; current_point.len()]; // Second moment
        let mut history = Vec::new();
        let mut iteration = 0;

        for iter in 0..self.max_iterations {
            iteration = iter;
            let function_value = problem.evaluate(&current_point);
            let gradient = problem.compute_gradient(&current_point);

            history.push(IterationData {
                iteration: iter,
                point: current_point.clone(),
                function_value,
                gradient_norm: gradient.iter().map(|&g| g * g).sum::<f64>().sqrt(),
            });

            // Check convergence
            let gradient_norm = gradient.iter().map(|&g| g * g).sum::<f64>().sqrt();
            if gradient_norm < self.tolerance {
                break;
            }

            // Adam update
            for i in 0..current_point.len() {
                // Update biased first moment estimate
                m[i] = self.beta1 * m[i] + (1.0 - self.beta1) * gradient[i];

                // Update biased second moment estimate
                v[i] = self.beta2 * v[i] + (1.0 - self.beta2) * gradient[i] * gradient[i];

                // Compute bias-corrected first moment estimate
                let m_hat = m[i] / (1.0 - self.beta1.powi((iter + 1) as i32));

                // Compute bias-corrected second moment estimate
                let v_hat = v[i] / (1.0 - self.beta2.powi((iter + 1) as i32));

                // Update parameters
                current_point[i] -= self.learning_rate * m_hat / (v_hat.sqrt() + self.epsilon);
            }
        }

        OptimizationResult {
            final_point: current_point,
            final_value: problem.evaluate(&history.last().unwrap().point),
            iterations: iteration + 1,
            converged: history.last().unwrap().gradient_norm < self.tolerance,
            history,
        }
    }
}

/// Newton's method with exact Hessian computation
#[derive(Debug, Clone)]
pub struct NewtonOptimizer {
    max_iterations: usize,
    tolerance: f64,
}

impl NewtonOptimizer {
    /// Create a new Newton optimizer
    pub fn new(max_iterations: usize, tolerance: f64) -> Self {
        Self {
            max_iterations,
            tolerance,
        }
    }

    /// Optimize using Newton's method (simplified for 2D problems)
    pub fn optimize(&self, problem: &OptimizationProblem, initial_point: Vec<f64>) -> OptimizationResult {
        let mut current_point = initial_point;
        let mut history = Vec::new();
        let mut iteration = 0;

        for iter in 0..self.max_iterations {
            iteration = iter;
            let function_value = problem.evaluate(&current_point);
            let gradient = problem.compute_gradient(&current_point);

            history.push(IterationData {
                iteration: iter,
                point: current_point.clone(),
                function_value,
                gradient_norm: gradient.iter().map(|&g| g * g).sum::<f64>().sqrt(),
            });

            // Check convergence
            let gradient_norm = gradient.iter().map(|&g| g * g).sum::<f64>().sqrt();
            if gradient_norm < self.tolerance {
                break;
            }

            // For 2D problems, compute Hessian using dual numbers
            if current_point.len() == 2 {
                let hessian = self.compute_hessian_2d(problem, &current_point);
                let det = hessian[0][0] * hessian[1][1] - hessian[0][1] * hessian[1][0];

                if det.abs() > 1e-10 {
                    // Newton step: x = x - H‚Åª¬π‚àáf
                    let inv_det = 1.0 / det;
                    let dx = -inv_det * (hessian[1][1] * gradient[0] - hessian[0][1] * gradient[1]);
                    let dy = -inv_det * (-hessian[1][0] * gradient[0] + hessian[0][0] * gradient[1]);

                    current_point[0] += dx;
                    current_point[1] += dy;
                } else {
                    // Fallback to gradient descent if Hessian is singular
                    current_point[0] -= 0.01 * gradient[0];
                    current_point[1] -= 0.01 * gradient[1];
                }
            } else {
                // Fallback to gradient descent for higher dimensions
                for (xi, gi) in current_point.iter_mut().zip(gradient.iter()) {
                    *xi -= 0.01 * gi;
                }
            }
        }

        OptimizationResult {
            final_point: current_point,
            final_value: problem.evaluate(&history.last().unwrap().point),
            iterations: iteration + 1,
            converged: history.last().unwrap().gradient_norm < self.tolerance,
            history,
        }
    }

    /// Compute 2x2 Hessian matrix using dual numbers
    fn compute_hessian_2d(&self, problem: &OptimizationProblem, point: &[f64]) -> [[f64; 2]; 2] {
        let epsilon = 1e-8;
        let mut hessian = [[0.0; 2]; 2];

        // Compute second partial derivatives numerically using first derivatives from dual numbers
        for i in 0..2 {
            for j in 0..2 {
                let mut point_plus = point.to_vec();
                let mut point_minus = point.to_vec();
                point_plus[j] += epsilon;
                point_minus[j] -= epsilon;

                let grad_plus = problem.compute_gradient(&point_plus);
                let grad_minus = problem.compute_gradient(&point_minus);

                hessian[i][j] = (grad_plus[i] - grad_minus[i]) / (2.0 * epsilon);
            }
        }

        hessian
    }
}

/// Data structure to store iteration results
#[derive(Debug, Clone)]
pub struct IterationData {
    pub iteration: usize,
    pub point: Vec<f64>,
    pub function_value: f64,
    pub gradient_norm: f64,
}

/// Result of an optimization run
#[derive(Debug, Clone)]
pub struct OptimizationResult {
    pub final_point: Vec<f64>,
    pub final_value: f64,
    pub iterations: usize,
    pub converged: bool,
    pub history: Vec<IterationData>,
}

/// Demonstrate gradient descent optimization
fn gradient_descent_demo() {
    println!("=== Gradient Descent with Exact Gradients ===");
    println!("Optimizing various functions using verified automatic differentiation\\n");

    let problems = vec![
        ("sphere", OptimizationProblem::new("sphere", 2)),
        ("rosenbrock", OptimizationProblem::new("rosenbrock", 2)),
        ("himmelblau", OptimizationProblem::new("himmelblau", 2)),
    ];

    let optimizer = GradientDescent::new(0.01, 1000, 1e-6);

    for (name, problem) in problems {
        println!("Optimizing {} function:", name);

        let initial_point = match name {
            "sphere" => vec![3.0, 2.0],
            "rosenbrock" => vec![-1.0, 1.0],
            "himmelblau" => vec![0.0, 0.0],
            _ => vec![1.0, 1.0],
        };

        println!("Initial point: ({:.3}, {:.3})", initial_point[0], initial_point[1]);

        let result = optimizer.optimize(&problem, initial_point);

        println!("Final point: ({:.6}, {:.6})", result.final_point[0], result.final_point[1]);
        println!("Final value: {:.8}", result.final_value);
        println!("Iterations: {}", result.iterations);
        println!("Converged: {}\\n", result.converged);

        // Show convergence trajectory (first few and last few iterations)
        if result.history.len() > 10 {
            println!("Convergence trajectory:");
            println!("Iter\\tPosition\\t\\t\\tFunction Value\\tGradient Norm");
            println!("{:-<70}", "");

            for i in 0..3 {
                let data = &result.history[i];
                println!("{}\\t({:.6}, {:.6})\\t\\t{:.6}\\t\\t{:.6}",
                    data.iteration, data.point[0], data.point[1], data.function_value, data.gradient_norm);
            }

            println!("...");

            for i in (result.history.len().saturating_sub(3))..result.history.len() {
                let data = &result.history[i];
                println!("{}\\t({:.6}, {:.6})\\t\\t{:.6}\\t\\t{:.6}",
                    data.iteration, data.point[0], data.point[1], data.function_value, data.gradient_norm);
            }
            println!();
        }
    }
}

/// Demonstrate Adam optimization
fn adam_optimization_demo() {
    println!("\\n=== Adam Optimization with Verified Gradients ===");
    println!("Advanced optimization using adaptive learning rates\\n");

    let problem = OptimizationProblem::new("rosenbrock", 2);
    let initial_point = vec![-1.2, 1.0];

    // Compare different optimizers
    let optimizers: Vec<(&str, Box<dyn Fn(&OptimizationProblem, Vec<f64>) -> OptimizationResult>)> = vec![
        ("Gradient Descent", Box::new(|p, init| {
            GradientDescent::new(0.001, 5000, 1e-6).optimize(p, init)
        })),
        ("Adam", Box::new(|p, init| {
            AdamOptimizer::new(0.1, 2000, 1e-6).optimize(p, init)
        })),
    ];

    println!("Optimizing Rosenbrock function: f(x,y) = (1-x)¬≤ + 100(y-x¬≤)¬≤");
    println!("Initial point: ({:.1}, {:.1})", initial_point[0], initial_point[1]);
    println!("Known global minimum: (1, 1) with f(1,1) = 0\\n");

    println!("Optimizer\\t\\tFinal Point\\t\\t\\tFinal Value\\tIterations\\tConverged");
    println!("{:-<90}", "");

    for (name, optimize_fn) in optimizers {
        let result = optimize_fn(&problem, initial_point.clone());

        println!("{}\\t\\t({:.6}, {:.6})\\t\\t{:.6}\\t{}\\t\\t{}",
            name,
            result.final_point[0], result.final_point[1],
            result.final_value,
            result.iterations,
            result.converged);
    }

    println!("\\nAdam typically converges faster due to adaptive learning rates");
    println!("and momentum, especially for non-convex optimization landscapes.");
}

/// Demonstrate Newton's method with Hessian
fn newton_method_demo() {
    println!("\\n=== Newton's Method with Exact Derivatives ===");
    println!("Second-order optimization using Hessian information\\n");

    let problem = OptimizationProblem::new("himmelblau", 2);
    let starting_points = vec![
        vec![0.0, 0.0],
        vec![-2.0, 2.0],
        vec![2.0, 2.0],
        vec![-2.0, -2.0],
    ];

    let newton = NewtonOptimizer::new(50, 1e-8);

    println!("Himmelblau's function: f(x,y) = (x¬≤+y-11)¬≤ + (x+y¬≤-7)¬≤");
    println!("Known minima: (3,2), (-2.8,3.1), (-3.8,-3.3), (3.6,-1.8)\\n");

    println!("Starting Point\\t\\tFinal Point\\t\\t\\tFinal Value\\tIterations");
    println!("{:-<80}", "");

    for initial_point in starting_points {
        let result = newton.optimize(&problem, initial_point.clone());

        println!("({:.1}, {:.1})\\t\\t\\t({:.6}, {:.6})\\t\\t{:.8}\\t{}",
            initial_point[0], initial_point[1],
            result.final_point[0], result.final_point[1],
            result.final_value,
            result.iterations);
    }

    println!("\\nNewton's method finds different local minima depending on");
    println!("the starting point, demonstrating the multimodal nature");
    println!("of Himmelblau's function.");
}

/// Demonstrate optimization on high-dimensional problems
fn high_dimensional_demo() {
    println!("\\n=== High-Dimensional Optimization ===");
    println!("Testing scalability with multi-dimensional functions\\n");

    let dimensions = vec![2, 5, 10, 20];
    let optimizer = AdamOptimizer::new(0.01, 1000, 1e-6);

    println!("Function: Rastrigin (highly multimodal)");
    println!("Dim\\tStarting Value\\t\\tFinal Value\\t\\tImprovement\\tIterations");
    println!("{:-<80}", "");

    for &dim in &dimensions {
        let problem = OptimizationProblem::new("rastrigin", dim);

        // Random starting point
        let mut rng = rand::thread_rng();
        let initial_point: Vec<f64> = (0..dim).map(|_| rng.gen::<f64>() * 10.0 - 5.0).collect();

        let initial_value = problem.evaluate(&initial_point);
        let result = optimizer.optimize(&problem, initial_point);

        let improvement = initial_value - result.final_value;
        let improvement_percent = (improvement / initial_value) * 100.0;

        println!("{}\\t{:.6}\\t\\t\\t{:.6}\\t\\t{:.1}% ({:.3})\\t{}",
            dim, initial_value, result.final_value, improvement_percent, improvement, result.iterations);
    }

    println!("\\nExact gradients from dual numbers enable stable optimization");
    println!("even in high-dimensional, highly multimodal landscapes.");
}

/// Demonstrate convergence analysis
fn convergence_analysis_demo() {
    println!("\\n=== Convergence Analysis ===");
    println!("Analyzing optimization convergence properties\\n");

    let problem = OptimizationProblem::new("sphere", 2);
    let initial_point = vec![5.0, 3.0];

    // Test different learning rates
    let learning_rates = vec![0.001, 0.01, 0.1, 0.3];

    println!("Sphere function optimization with different learning rates:");
    println!("Learning Rate\\tFinal Error\\t\\tIterations\\tConverged\\tStability");
    println!("{:-<80}", "");

    for &lr in &learning_rates {
        let optimizer = GradientDescent::new(lr, 1000, 1e-8);
        let result = optimizer.optimize(&problem, initial_point.clone());

        let final_error = result.final_value.sqrt(); // Distance from optimum
        let stability = if result.history.len() > 10 {
            let last_10: Vec<f64> = result.history.iter()
                .rev().take(10)
                .map(|h| h.function_value)
                .collect();

            let mean = last_10.iter().sum::<f64>() / last_10.len() as f64;
            let variance = last_10.iter()
                .map(|&x| (x - mean).powi(2))
                .sum::<f64>() / last_10.len() as f64;

            if variance.sqrt() < 1e-10 { "Stable" } else { "Unstable" }
        } else {
            "Unknown"
        };

        println!("{:.3}\\t\\t\\t{:.8}\\t\\t{}\\t\\t{}\\t\\t{}",
            lr, final_error, result.iterations, result.converged, stability);
    }

    println!("\\nOptimal learning rates balance convergence speed with stability.");
    println!("Exact gradients enable precise convergence analysis.");
}

fn main() {
    println!("ELECTROMAGNETIC Optimization Algorithms with Verified Mathematics");
    println!("==================================================\\n");

    println!("This example demonstrates optimization algorithms using");
    println!("dual numbers for exact gradient computation:\\n");

    println!("‚Ä¢ Gradient descent with machine-precision gradients");
    println!("‚Ä¢ Adam optimizer with verified adaptive updates");
    println!("‚Ä¢ Newton's method with exact Hessian computation");
    println!("‚Ä¢ High-dimensional optimization scalability");
    println!("‚Ä¢ Convergence analysis and stability testing");
    println!("‚Ä¢ Comparison with traditional finite difference methods\\n");

    // Run the demonstrations
    gradient_descent_demo();
    adam_optimization_demo();
    newton_method_demo();
    high_dimensional_demo();
    convergence_analysis_demo();

    println!("\\n=== Advantages of Verified Optimization ==");
    println!("1. Exact gradients eliminate numerical approximation errors");
    println!("2. Stable convergence even with challenging functions");
    println!("3. Reliable performance in high-dimensional spaces");
    println!("4. Mathematical guarantees on gradient accuracy");
    println!("5. Robust optimization for safety-critical applications");
    println!("6. Foundation for provably correct algorithms");

    println!("\\nüéì Educational Value:");
    println!("Dual number automatic differentiation provides the");
    println!("mathematical foundation for verified optimization,");
    println!("enabling robust and reliable machine learning systems.");
}